{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model training\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Enable MPS (Apple Silicon) for PyTorch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>explanation</th>\n",
       "      <th>fact_checkers</th>\n",
       "      <th>main_text</th>\n",
       "      <th>sources</th>\n",
       "      <th>label</th>\n",
       "      <th>subjects</th>\n",
       "      <th>claim_cleaned</th>\n",
       "      <th>explanation_cleaned</th>\n",
       "      <th>main_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15661</td>\n",
       "      <td>\"The money the Clinton Foundation took from fr...</td>\n",
       "      <td>April 26, 2015</td>\n",
       "      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n",
       "      <td>Katie Sanders</td>\n",
       "      <td>\"Hillary Clinton is in the political crosshair...</td>\n",
       "      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n",
       "      <td>false</td>\n",
       "      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n",
       "      <td>the money the clinton foundation took from fro...</td>\n",
       "      <td>gingrich said the clinton foundation took mone...</td>\n",
       "      <td>hillary clinton is in the political crosshairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9893</td>\n",
       "      <td>Annual Mammograms May Have More False-Positives</td>\n",
       "      <td>October 18, 2011</td>\n",
       "      <td>This article reports on the results of a study...</td>\n",
       "      <td></td>\n",
       "      <td>While the financial costs of screening mammogr...</td>\n",
       "      <td></td>\n",
       "      <td>mixture</td>\n",
       "      <td>Screening,WebMD,women's health</td>\n",
       "      <td>annual mammograms may have more falsepositives</td>\n",
       "      <td>this article reports on the results of a study...</td>\n",
       "      <td>while the financial costs of screening mammogr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11358</td>\n",
       "      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n",
       "      <td>September 28, 2016</td>\n",
       "      <td>This news release describes five-year outcomes...</td>\n",
       "      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n",
       "      <td>The news release quotes lead researcher Robert...</td>\n",
       "      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n",
       "      <td>mixture</td>\n",
       "      <td>Association/Society news release,Cancer</td>\n",
       "      <td>sbrt offers prostate cancer patients high canc...</td>\n",
       "      <td>this news release describes fiveyear outcomes ...</td>\n",
       "      <td>the news release quotes lead researcher robert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10166</td>\n",
       "      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n",
       "      <td>November 8, 2011</td>\n",
       "      <td>While the story does many things well, the ove...</td>\n",
       "      <td></td>\n",
       "      <td>The story does discuss costs, but the framing ...</td>\n",
       "      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n",
       "      <td>true</td>\n",
       "      <td>Cancer,WebMD,women's health</td>\n",
       "      <td>study vaccine for breast ovarian cancer has po...</td>\n",
       "      <td>while the story does many things well the over...</td>\n",
       "      <td>the story does discuss costs but the framing i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11276</td>\n",
       "      <td>Some appendicitis cases may not require ’emerg...</td>\n",
       "      <td>September 20, 2010</td>\n",
       "      <td>We really don’t understand why only a handful ...</td>\n",
       "      <td></td>\n",
       "      <td>\"Although the story didn’t cite the cost of ap...</td>\n",
       "      <td></td>\n",
       "      <td>true</td>\n",
       "      <td></td>\n",
       "      <td>some appendicitis cases may not require ’emerg...</td>\n",
       "      <td>we really don’t understand why only a handful ...</td>\n",
       "      <td>although the story didn’t cite the cost of app...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim  \\\n",
       "0    15661  \"The money the Clinton Foundation took from fr...   \n",
       "1     9893    Annual Mammograms May Have More False-Positives   \n",
       "2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n",
       "3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n",
       "4    11276  Some appendicitis cases may not require ’emerg...   \n",
       "\n",
       "       date_published                                        explanation  \\\n",
       "0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n",
       "1    October 18, 2011  This article reports on the results of a study...   \n",
       "2  September 28, 2016  This news release describes five-year outcomes...   \n",
       "3    November 8, 2011  While the story does many things well, the ove...   \n",
       "4  September 20, 2010  We really don’t understand why only a handful ...   \n",
       "\n",
       "                                       fact_checkers  \\\n",
       "0                                      Katie Sanders   \n",
       "1                                                      \n",
       "2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                           main_text  \\\n",
       "0  \"Hillary Clinton is in the political crosshair...   \n",
       "1  While the financial costs of screening mammogr...   \n",
       "2  The news release quotes lead researcher Robert...   \n",
       "3  The story does discuss costs, but the framing ...   \n",
       "4  \"Although the story didn’t cite the cost of ap...   \n",
       "\n",
       "                                             sources    label  \\\n",
       "0  https://www.wsj.com/articles/clinton-foundatio...    false   \n",
       "1                                                     mixture   \n",
       "2  https://www.healthnewsreview.org/wp-content/up...  mixture   \n",
       "3  http://clinicaltrials.gov/ct2/results?term=can...     true   \n",
       "4                                                        true   \n",
       "\n",
       "                                      subjects  \\\n",
       "0  Foreign Policy, PunditFact, Newt Gingrich,    \n",
       "1               Screening,WebMD,women's health   \n",
       "2      Association/Society news release,Cancer   \n",
       "3                  Cancer,WebMD,women's health   \n",
       "4                                                \n",
       "\n",
       "                                       claim_cleaned  \\\n",
       "0  the money the clinton foundation took from fro...   \n",
       "1     annual mammograms may have more falsepositives   \n",
       "2  sbrt offers prostate cancer patients high canc...   \n",
       "3  study vaccine for breast ovarian cancer has po...   \n",
       "4  some appendicitis cases may not require ’emerg...   \n",
       "\n",
       "                                 explanation_cleaned  \\\n",
       "0  gingrich said the clinton foundation took mone...   \n",
       "1  this article reports on the results of a study...   \n",
       "2  this news release describes fiveyear outcomes ...   \n",
       "3  while the story does many things well the over...   \n",
       "4  we really don’t understand why only a handful ...   \n",
       "\n",
       "                                   main_text_cleaned  \n",
       "0  hillary clinton is in the political crosshairs...  \n",
       "1  while the financial costs of screening mammogr...  \n",
       "2  the news release quotes lead researcher robert...  \n",
       "3  the story does discuss costs but the framing i...  \n",
       "4  although the story didn’t cite the cost of app...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'National, Candidate Biography, Donald Trump, ': np.int64(0), 'false': np.int64(1), 'mixture': np.int64(2), 'snopes': np.int64(3), 'true': np.int64(4), 'unproven': np.int64(5), nan: np.int64(6)}\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned datasets\n",
    "train_df = pd.read_csv('data/processed/train_cleaned.csv')\n",
    "dev_df = pd.read_csv('data/processed/dev_cleaned.csv')\n",
    "test_df = pd.read_csv('data/processed/test_cleaned.csv')\n",
    "\n",
    "# Display the first few rows of the training set\n",
    "print(\"Cleaned Train Dataset:\")\n",
    "display(train_df.head())\n",
    "\n",
    "# Combine all labels before encoding\n",
    "all_labels = pd.concat([train_df['label'], dev_df['label'], test_df['label']]).unique()\n",
    "\n",
    "# Fit LabelEncoder on all unique labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Transform labels for each dataset\n",
    "train_df['label_encoded'] = label_encoder.transform(train_df['label'])\n",
    "dev_df['label_encoded'] = label_encoder.transform(dev_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# Display label mapping\n",
    "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define custom dataset class for BERT\n",
    "class PubHealthDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 8  # Reduce batch size for MPS optimization\n",
    "\n",
    "train_dataset = PubHealthDataset(\n",
    "    texts=train_df['claim_cleaned'].values,\n",
    "    labels=train_df['label_encoded'].values,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "dev_dataset = PubHealthDataset(\n",
    "    texts=dev_df['claim_cleaned'].values,\n",
    "    labels=dev_df['label_encoded'].values,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/3 =====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     22\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrect_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mdataset), np\u001b[38;5;241m.\u001b[39mmean(losses)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n===== Epoch {epoch + 1}/{EPOCHS} =====')\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    print(f'Train Loss: {train_loss} | Train Accuracy: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the trained BERT model\n",
    "model_save_path = 'models/bert_pubhealth.pt'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Trained model saved at {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
